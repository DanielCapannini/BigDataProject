{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Project Big Data (2024/2025)",
   "id": "bc0bc68eab982185"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-11T10:10:26.898430Z",
     "start_time": "2025-01-11T10:10:12.614134Z"
    }
   },
   "source": "import org.apache.spark",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://127.0.0.1:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1736590217756)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "// DO NOT EXECUTE - this is needed just to avoid showing errors in the following cells\n",
    "val sc = spark.SparkContext.getOrCreate()"
   ],
   "id": "39a8ac5f9dc092ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T10:10:33.764739Z",
     "start_time": "2025-01-11T10:10:32.006141Z"
    }
   },
   "cell_type": "code",
   "source": "val spark = org.apache.spark.sql.SparkSession.builder.appName(\"BigDataExam\").getOrCreate()",
   "id": "f3d937b885f2b577",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@14be94b4\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parser for IMDb datasets",
   "id": "2e2c37b07577bc46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T10:10:37.835202Z",
     "start_time": "2025-01-11T10:10:36.438306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.util.Calendar\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "object IMDbParser{\n",
    "\n",
    "    def parseAkasLine(line: String): Option[\n",
    "        (String, Int, String, String, String, Array[String], Array[String], Boolean)] = {\n",
    "        val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "        val input = line.split(commaRegex).map(_.trim.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "        try {\n",
    "            Some(\n",
    "            input(0), // titleId\n",
    "            input(1).toInt, // ordering\n",
    "            input(2), // title\n",
    "            if (input(3) == \"\\\\N\") \"\" else input(3), // region\n",
    "            if (input(4) == \"\\\\N\") \"\" else input(4), // language\n",
    "            if (input(5) == \"\\\\N\") Array.empty[String] else input(5).split(\",\").map(_.trim), // types as Array[String]\n",
    "            if (input(6) == \"\\\\N\") Array.empty[String] else input(6).split(\",\").map(_.trim), // attributes as Array[String]\n",
    "            input(7) == \"1\" // isOriginalTitle\n",
    "            )\n",
    "        } catch {\n",
    "            case e: Exception => None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def parseCrewLine(line: String): Option[(String, Array[String], Array[String])] = {\n",
    "        val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "        val input = line.split(commaRegex).map(_.trim.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "        try {\n",
    "            Some(\n",
    "                input(0), // tconst\n",
    "                if (input(1) == \"\\\\N\") Array.empty[String] else input(1).split(\",\").map(_.trim), // directors as Array[String]\n",
    "                if (input(2) == \"\\\\N\") Array.empty[String] else input(2).split(\",\").map(_.trim) // writers as Array[String]\n",
    "            )\n",
    "        } catch {\n",
    "        case e: Exception => None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def parseEpisodeLine(line: String): Option[(String, String, Int, Int)] = {\n",
    "        val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "        val input = line.split(commaRegex).map(_.trim.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "        try {\n",
    "            Some(\n",
    "                input(0), // tconst\n",
    "                input(1), // parentTconst\n",
    "                if (input(2) == \"\\\\N\") 0 else input(2).toInt, // seasonNumber\n",
    "                if (input(3) == \"\\\\N\") 0 else input(3).toInt // episodeNumber\n",
    "            )\n",
    "        } catch {\n",
    "            case e: Exception => None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def parsePrincipalsLine(line: String): Option[(String, Int, String, String, String, String)] = {\n",
    "        val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "        val input = line.split(commaRegex).map(_.trim.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "        try {\n",
    "            Some(\n",
    "                input(0), // tconst\n",
    "                input(1).toInt, // ordering\n",
    "                input(2), // nconst\n",
    "                input(3), // category\n",
    "                if (input(4) == \"\\\\N\") \"\" else input(4), // job\n",
    "                if (input(5) == \"\\\\N\") \"\" else input(5) // characters\n",
    "            )\n",
    "        } catch {\n",
    "            case e: Exception => None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def parseRatingsLine(line: String): Option[(String, Double, Int)] = {\n",
    "        val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "        val input = line.split(commaRegex).map(_.trim.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "        try {\n",
    "            Some(\n",
    "                input(0), // tconst\n",
    "                input(1).toDouble, // averageRating\n",
    "                input(2).toInt // numVotes\n",
    "            )\n",
    "        } catch {\n",
    "            case e: Exception => None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def parseNameBasicsLine(line: String): Option[(String, String, Int, Int, Array[String], Array[String])] = {\n",
    "        val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "        val input = line.split(commaRegex).map(_.trim.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "        try {\n",
    "            Some(\n",
    "                input(0), // nconst\n",
    "                input(1), // primaryName\n",
    "                input(2).toInt, // birthYear\n",
    "                if (input(3) == \"\\\\N\") 0 else input(3).toInt, // deathYear\n",
    "                input(4).split(\",\").map(_.trim), // primaryProfession as Array[String]\n",
    "                input(5).split(\",\").map(_.trim) // knownForTitles as Array[String]\n",
    "            )\n",
    "        } catch {\n",
    "            case e: Exception => None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def parseTitleBasicsLine(line: String): Option[\n",
    "        (String, String, String, String, Boolean, Int, Int, Int, Array[String])] = {\n",
    "        val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "        val input = line.split(commaRegex).map(_.trim.replaceAll(\"^\\\"|\\\"$\", \"\"))\n",
    "        try {\n",
    "            Some(\n",
    "                input(0), // tconst\n",
    "                input(1), // titleType\n",
    "                input(2), // primaryTitle\n",
    "                input(3), // originalTitle\n",
    "                input(4) == \"1\", // isAdult\n",
    "                input(5).toInt, // startYear\n",
    "                if (input(6) == \"\\\\N\") 0 else input(6).toInt, // endYear\n",
    "                if (input(7) == \"\\\\N\") 0 else input(7).toInt, // runtimeMinutes\n",
    "                input(8).split(\",\").map(_.trim) // genres as Array[String]\n",
    "            )\n",
    "        } catch {\n",
    "            case e: Exception => None\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "id": "5cfc49b1e82b7a5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Calendar\r\n",
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.spark.HashPartitioner\r\n",
       "defined object IMDbParser\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T10:10:51.783740Z",
     "start_time": "2025-01-11T10:10:50.098122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddTitleBasics = sc.textFile(\"../../../../datasets/project/csv/title.basics_half.csv\").flatMap(IMDbParser.parseTitleBasicsLine)\n",
    "val rddPrincipals = sc.textFile(\"../../../../datasets/project/csv/title.principals_half.csv\").flatMap(IMDbParser.parsePrincipalsLine)\n",
    "val rddRatings = sc.textFile(\"../../../../datasets/project/csv/title.ratings_half.csv\").flatMap(IMDbParser.parseRatingsLine)\n",
    "val rddNameBasics = sc.textFile(\"../../../../datasets/project/csv/name.basics.csv\").flatMap(IMDbParser.parseNameBasicsLine)"
   ],
   "id": "fdd3c5415dd63246",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddTitleBasics: org.apache.spark.rdd.RDD[(String, String, String, String, Boolean, Int, Int, Int, Array[String])] = MapPartitionsRDD[2] at flatMap at <console>:29\r\n",
       "rddPrincipals: org.apache.spark.rdd.RDD[(String, Int, String, String, String, String)] = MapPartitionsRDD[5] at flatMap at <console>:30\r\n",
       "rddRatings: org.apache.spark.rdd.RDD[(String, Double, Int)] = MapPartitionsRDD[8] at flatMap at <console>:31\r\n",
       "rddNameBasics: org.apache.spark.rdd.RDD[(String, String, Int, Int, Array[String], Array[String])] = MapPartitionsRDD[11] at flatMap at <console>:32\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Valutazione dei file del dataset che vengono usati",
   "id": "1f7ee245a512b2c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### dimenzione file originali",
   "id": "1050ad7ec8f25e5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![dimezione file originali](resorc/graf4.png)",
   "id": "1849bab7eb614367"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### dimenzione file ridotti per l'elaborazione locale",
   "id": "616e1b061de23c06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![dimezione file ridotti per l'elaborazione locale](resorc/graf5.png)",
   "id": "871b41ee6ce366ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Job: Valutazione Attori per Genere\n",
    "\n",
    "### **Metriche Considerate**\n",
    "1. **Valutazione del film**:\n",
    "   - Vengono considerate solo le valutazioni di film con più di 500 valutazioni.\n",
    "2. **Genere**:\n",
    "   - Genere cinematografico dei film.\n",
    "3. **Nome Attore**:\n",
    "   - Nome dell'attore che ha partecipato ai film.\n",
    "4. **Film Partecipati**:\n",
    "   - Elenco dei film a cui l'attore ha partecipato.\n",
    "\n",
    "### **Obiettivo del Job**\n",
    "Valutare ogni attore per ogni genere, assegnando una classificazione basata sui seguenti criteri:\n",
    "\n",
    "#### **Criteri di Valutazione**\n",
    "1. **Inclassificabile**:\n",
    "   - Se l'attore non ha partecipato a nessun film di quel genere.\n",
    "2. **Insufficiente**:\n",
    "   - Se l'attore ha partecipato a meno di 2 film di quel genere **oppure** ha una valutazione media inferiore a 6.\n",
    "3. **Sufficiente**:\n",
    "   - Se l'attore ha partecipato ad almeno 2 film di quel genere **oppure** ha una valutazione media superiore a 6.\n",
    "\n",
    "---\n",
    "\n",
    "## Job0\n",
    "\n",
    "vesione senza alcuna ottimizzazione tranne per la groupByKey e la seguente valutazione eseguita nella prima occazione possibile evitando di eseguire ulteriori join prima della valutazione.\n"
   ],
   "id": "1c8ce422cd3389c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T10:14:46.640613Z",
     "start_time": "2025-01-11T10:10:59.732137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val actorsAndActresses = rddNameBasics.filter { case (_, _, _, _, primaryProfession, _) =>\n",
    "    primaryProfession.contains(\"actor\") || primaryProfession.contains(\"actress\")\n",
    "    }.filter(_._4 == 0).map { case (nconst, primaryName, _, _, _, _) => (nconst, primaryName) }\n",
    "\n",
    "val moviesRating = rddPrincipals.map { case (tconst, _, nconst, category, _, _) => (tconst, nconst) }\n",
    "    .join(rddRatings.filter(_._3 > 500).map { case (tconst, rating, _) => (tconst, rating) })\n",
    "\n",
    "val actorGenreRatings = rddTitleBasics.filter(_._2 == \"movie\").flatMap {\n",
    "    case (tconst, _, _, _, _, _, _, _, genres) => genres.map(genre => (tconst, genre))}\n",
    "    .join(moviesRating)\n",
    "    .map { case (tconst, (genre, (nconst, rating))) => ((nconst, genre), rating) }\n",
    "    .groupByKey()\n",
    "    .mapValues(ratings => (ratings.sum / ratings.size, ratings.size))\n",
    "\n",
    "val job = actorsAndActresses.cartesian(rddTitleBasics.filter(_._2 == \"movie\").flatMap(_._9).filter(x => x!= \"\\\\N\").distinct())\n",
    "    .leftOuterJoin(actorGenreRatings)\n",
    "    .map { case ((nconst, primaryName), (genre, maybeRatings)) =>\n",
    "        val sufficiency = maybeRatings match {\n",
    "            case Some((avgRating, count)) => if (avgRating > 6 && count >= 2) \"sufficiente\" else \"insufficiente\"\n",
    "            case None => \"inclassificabile\"\n",
    "        }\n",
    "    (nconst, primaryName, genre, sufficiency)\n",
    "    }\n",
    "    \n",
    "spark.time {\n",
    "    job.collect()\n",
    "}"
   ],
   "id": "a8ce2ed1931af6bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 225180 ms\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "actorsAndActresses: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[14] at map at <console>:35\r\n",
       "moviesRating: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[20] at join at <console>:38\r\n",
       "actorGenreRatings: org.apache.spark.rdd.RDD[((String, String), (Double, Int))] = MapPartitionsRDD[28] at mapValues at <console>:45\r\n",
       "job: org.apache.spark.rdd.RDD[(String, String, String, String)] = MapPartitionsRDD[39] at map at <console>:49\r\n",
       "res0: Array[(String, String, String, String)] = Array((nm0360025,Jay Hanks,War,inclassificabile), (nm0360025,Jay Hanks,Fantasy,inclassificabile), (nm0360025,Jay Hanks,Western,inclassificabile), (nm0360025,Jay Hanks,Musical,inclassificabile), (nm0360025,Jay Hanks,Family,inclassificabile), (nm0360025,Jay Hanks,Horror,inclassifica...\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Job1\n",
    "\n",
    "versione con ottimizzazione tramite l'utilizzo di broadcast variables per evitare di eseguire join riducendo cosi gli shuffle che devono essere (in questo caso vi è un utilizzo eccessivo delle broadcast variables).\n",
    "\n",
    "è stato anche provato a utilizzare il metodo aggregateByKey per calcolare la media e il conteggio delle valutazioni per ogni attore e genere e a seguire gia una valutazione sul per assognare la valutazione agli attori."
   ],
   "id": "5b86780fc45f6564"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T10:15:59.906465Z",
     "start_time": "2025-01-11T10:14:55.023152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddMovie = rddTitleBasics.filter(_._2 == \"movie\").flatMap {\n",
    "    case (tconst, _, _, _, _, _, _, _, genres) => genres.map(genre => (tconst, genre))}\n",
    "\n",
    "val actorsAndActresses = rddNameBasics.filter { case (_, _, _, _, primaryProfession, _) =>\n",
    "    primaryProfession.contains(\"actor\") || primaryProfession.contains(\"actress\")\n",
    "    }.filter(_._4 == 0).map { case (nconst, primaryName, _, _, _, _) => (nconst, primaryName) }\n",
    "\n",
    "val broadcastRating = sc.broadcast(rddRatings.filter(_._3 > 500).map { case (tconst, rating, _) => (tconst, rating)}.collectAsMap())\n",
    "\n",
    "val moviesRating = sc.broadcast(rddPrincipals.filter { case (_, _, _, primaryProfession, _, _) =>\n",
    "    primaryProfession.contains(\"actor\") || primaryProfession.contains(\"actress\")\n",
    "    }.map { case (tconst, _, nconst, category, _, _) => (tconst, nconst) }\n",
    "    .flatMap { case (tconst, nconst) =>\n",
    "        broadcastRating.value.get(tconst) match {\n",
    "            case Some(rating) => Some(tconst, (nconst, rating))\n",
    "            case None => None\n",
    "        }\n",
    "    }.collectAsMap())\n",
    "\n",
    "val actorGenreRatings = sc.broadcast(rddMovie.flatMap { case (tconst, genre) =>\n",
    "    moviesRating.value.get(tconst) match {\n",
    "        case Some((nconst, rating)) => Some((nconst, genre), rating)\n",
    "        case None => None\n",
    "    }\n",
    "    }.aggregateByKey((0.0, 0))(\n",
    "        (acc, rating) => (acc._1 + rating, acc._2 + 1),\n",
    "        (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "    ).map{\n",
    "        case ((nconst, genre), (sum, count)) =>\n",
    "        val sufficiency = if (sum / count > 6 && count >= 2) \"sufficiente\" else \"insufficiente\"\n",
    "        ((nconst, genre), sufficiency)\n",
    "    }.collectAsMap())\n",
    "\n",
    "val broadcastGenres = sc.broadcast(rddTitleBasics.filter(_._2 == \"movie\").flatMap(_._9).filter(x => x!= \"\\\\N\").distinct().collect())\n",
    "\n",
    "val job1 = actorsAndActresses.flatMap { case (nconst, primaryName) =>\n",
    "    broadcastGenres.value.map(genre => ((nconst, genre), primaryName))\n",
    "    }.map{ case ((nconst, genre), primaryName) =>\n",
    "    actorGenreRatings.value.get((nconst, genre)) match {\n",
    "        case Some(sufficiency) => (nconst, primaryName, genre, sufficiency)\n",
    "        case None => (nconst, primaryName, genre, \"inclassificabile\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "spark.time {\n",
    "    job1.collect()\n",
    "}"
   ],
   "id": "5d982bc2df646d1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 60535 ms\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rddMovie: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[41] at flatMap at <console>:37\r\n",
       "actorsAndActresses: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[44] at map at <console>:42\r\n",
       "broadcastRating: org.apache.spark.broadcast.Broadcast[scala.collection.Map[String,Double]] = Broadcast(13)\r\n",
       "moviesRating: org.apache.spark.broadcast.Broadcast[scala.collection.Map[String,(String, Double)]] = Broadcast(15)\r\n",
       "actorGenreRatings: org.apache.spark.broadcast.Broadcast[scala.collection.Map[(String, String),String]] = Broadcast(18)\r\n",
       "broadcastGenres: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(21)\r\n",
       "job1: org.apache.spark.rdd.RDD[(String, String, String, String)] = MapPartitionsRDD[60] at map at <console>:74\r\n",
       "res1: Array[(String, String, String, Strin...\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T10:16:09.881343Z",
     "start_time": "2025-01-11T10:16:09.501866Z"
    }
   },
   "cell_type": "code",
   "source": "import org.apache.spark.HashPartitioner",
   "id": "c02bbed676529a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Job2\n",
    "\n",
    "questa versione non utiliazza le broadcast variables ma utilizza un HashPartitioner.\n",
    "\n",
    "ci si è soffermati sul rimuovere l'operazione di cartesian la quale risulta essere la più costosa in termini di tempo e di risorse."
   ],
   "id": "4ce41bd15019fefa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T10:17:41.126730Z",
     "start_time": "2025-01-11T10:16:12.397399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val p = new HashPartitioner(5)\n",
    "\n",
    "val distinctGenres = rddTitleBasics.filter(_._2 == \"movie\").flatMap(_._9).filter(x => x!= \"\\\\N\").distinct().collect()\n",
    "\n",
    "val actorsAndActresses = rddNameBasics.filter { case (_, _, _, _, primaryProfession, _) =>\n",
    "    primaryProfession.contains(\"actor\") || primaryProfession.contains(\"actress\")\n",
    "    }.filter(_._4 == 0).map { case (nconst, primaryName, _, _, _, _) => (nconst, primaryName) }\n",
    "\n",
    "val moviesRating = rddPrincipals.filter { case (_, _, _, primaryProfession, _, _) =>\n",
    "    primaryProfession.contains(\"actor\") || primaryProfession.contains(\"actress\")\n",
    "    }.map { case (tconst, _, nconst, category, _, _) => (tconst, nconst) }\n",
    "    .join(rddRatings.filter(_._3 > 500).map { case (tconst, rating, _) => (tconst, rating) }.partitionBy(p))\n",
    "\n",
    "val actorGenreRatings = rddTitleBasics.filter(_._2 == \"movie\").flatMap {\n",
    "    case (tconst, _, _, _, _, _, _, _, genres) => genres.map(genre => (tconst, genre))\n",
    "    }.join(moviesRating.partitionBy(p))\n",
    "    .map { case (tconst, (genre, (nconst, rating))) => ((nconst, genre), rating) }\n",
    "    .groupByKey()\n",
    "    .mapValues(ratings => (ratings.sum / ratings.size, ratings.size))\n",
    "\n",
    "val job2 = actorsAndActresses.flatMap { case (nconst, primaryName) =>\n",
    "    distinctGenres.map(genre => (nconst, primaryName, genre))\n",
    "    }.map { case (nconst, primaryName, genre) =>\n",
    "        ((nconst, genre), (primaryName, genre))\n",
    "    }.leftOuterJoin(actorGenreRatings)\n",
    "    .map { case ((nconst, genre), ((primaryName, _), maybeRatings)) =>\n",
    "        val sufficiency = maybeRatings match {\n",
    "        case Some((avgRating, count)) =>\n",
    "        if (avgRating > 6 && count >= 2) \"sufficiente\" else \"insufficiente\"\n",
    "        case None => \"inclassificabile\"\n",
    "    }\n",
    "    (nconst, primaryName, genre, sufficiency)\n",
    "    }\n",
    "            \n",
    "spark.time {\n",
    "    job2.collect()\n",
    "}"
   ],
   "id": "ae7396d17e52759b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 86585 ms\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "p: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@5\r\n",
       "distinctGenres: Array[String] = Array(War, Fantasy, Western, Musical, Family, Horror, Crime, Animation, Sport, Adult, History, Thriller, Adventure, Talk-Show, Action, Music, News, Biography, Sci-Fi, Comedy, Documentary, Mystery, Reality-TV, Romance, Drama, Film-Noir)\r\n",
       "actorsAndActresses: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[69] at map at <console>:44\r\n",
       "moviesRating: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[77] at join at <console>:49\r\n",
       "actorGenreRatings: org.apache.spark.rdd.RDD[((String, String), (Double, Int))] = MapPartitionsRDD[85] at mapValues at <console>:56\r\n",
       "job2: org.apache.spark.rdd.RDD[(String, String, String, String)] = MapPartitionsRDD[91] at map at...\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Job3\n",
    "\n",
    "questa versione è sostanzialmente uguale alla precedente ma si è utilizzato un broadcast variable per sostituire le join che implicavano rdd di grandi dimenzioni."
   ],
   "id": "14e3e4ee85a43da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T10:19:37.498731Z",
     "start_time": "2025-01-11T10:17:47.813977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val distinctGenres = sc.broadcast(rddTitleBasics.filter(_._2 == \"movie\").flatMap(_._9).filter(x => x != \"\\\\N\").distinct().collect())\n",
    "\n",
    "val actorGenrePairs = rddNameBasics.filter { case (_, _, _, _, primaryProfession, _) =>\n",
    "    primaryProfession.contains(\"actor\") || primaryProfession.contains(\"actress\")\n",
    "    }.filter(_._4 == 0).map { case (nconst, primaryName, _, _, _, _) => (nconst, primaryName) }.partitionBy(p).flatMap { case (nconst, primaryName) =>\n",
    "    distinctGenres.value.map(genre => (nconst, primaryName, genre))\n",
    "    }\n",
    "\n",
    "val ratingBroadcast = sc.broadcast(rddRatings.filter(_._3 > 500).map { case (tconst, rating, _) => (tconst, rating) }.collectAsMap())\n",
    "\n",
    "val moviesRating = rddPrincipals.filter { case (_, _, _, primaryProfession, _, _) =>\n",
    "    primaryProfession.contains(\"actor\") || primaryProfession.contains(\"actress\")\n",
    "    }.flatMap { case (tconst, _, nconst, category, _, _) => ratingBroadcast.value.get(tconst).map(rating => (tconst, (nconst, rating))) }\n",
    "\n",
    "val actorGenreRatings = rddTitleBasics.filter(_._2 == \"movie\").flatMap {\n",
    "    case (tconst, _, _, _, _, _, _, _, genres) => genres.map(genre => (tconst, genre))\n",
    "    }.join(moviesRating.partitionBy(p))\n",
    "    .map { case (tconst, (genre, (nconst, rating))) => ((nconst, genre), rating) }\n",
    "    .groupByKey()\n",
    "    .mapValues(ratings => (ratings.sum / ratings.size, ratings.size))\n",
    "\n",
    "val job3 = actorGenrePairs.map { case (nconst, primaryName, genre) =>\n",
    "        ((nconst, genre), (primaryName, genre))\n",
    "    }.leftOuterJoin(actorGenreRatings)\n",
    "    .map { case ((nconst, genre), ((primaryName, _), maybeRatings)) =>\n",
    "        val sufficiency = maybeRatings match {\n",
    "        case Some((avgRating, count)) =>\n",
    "        if (avgRating > 6 && count >= 2) \"sufficiente\" else \"insufficiente\"\n",
    "        case None => \"inclassificabile\"\n",
    "    }\n",
    "    (nconst, primaryName, genre, sufficiency)\n",
    "    }\n",
    "            \n",
    "spark.time {\n",
    "    job3.collect()\n",
    "}"
   ],
   "id": "77e2fdc1397beabb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 107267 ms\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "distinctGenres: org.apache.spark.broadcast.Broadcast[Array[String]] = Broadcast(33)\r\n",
       "actorGenrePairs: org.apache.spark.rdd.RDD[(String, String, String)] = MapPartitionsRDD[102] at flatMap at <console>:44\r\n",
       "ratingBroadcast: org.apache.spark.broadcast.Broadcast[scala.collection.Map[String,Double]] = Broadcast(35)\r\n",
       "moviesRating: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[106] at flatMap at <console>:52\r\n",
       "actorGenreRatings: org.apache.spark.rdd.RDD[((String, String), (Double, Int))] = MapPartitionsRDD[115] at mapValues at <console>:59\r\n",
       "job3: org.apache.spark.rdd.RDD[(String, String, String, String)] = MapPartitionsRDD[120] at map at <console>:64\r\n",
       "res3: Array[(String, String, String, String)] = Array((nm0769925,Djamchid 'Jim' Soheili,Musical,inclassificabile), (nm4...\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Valutazione dei job in esecuzione sulla macchina locale",
   "id": "91945552d2903c30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Grafico dei processi](resorc/graf1.png)",
   "id": "1b00f4da0225d7ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Confronto delle Prestazioni**\n",
    "Rispetto a **Job0**, sono stati ottenuti discreti miglioramenti in termini di velocità di esecuzione:\n",
    "\n",
    "1. **Job1**:\n",
    "   - Risulta **3,72 volte più veloce** di Job0.\n",
    "\n",
    "2. **Job2**:\n",
    "   - Risulta **2,60 volte più veloce** di Job0.\n",
    "\n",
    "3. **Job3**:\n",
    "   - Risulta **2,10 volte più veloce** di Job0.\n",
    "      \n",
    "\n",
    "è risultato inaspettato che il Job1 fosse poi efficente del Job2 e Job3, questo probabilmente è dovuto alla riduzione del dataset che è stata eseguita per poter eseguire i Jobs sul dispositivo locale.\n"
   ],
   "id": "7d01be10634a45a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Valutazione dei job in esecuzione sul cluster",
   "id": "255543da8613b332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "aws emr create-cluster \\\n",
    "    --name \"Big Data Cluster\" \\\n",
    "    --release-label \"emr-7.3.0\" \\\n",
    "    --applications Name=Hadoop Name=Spark \\\n",
    "    --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m4.large InstanceGroupType=CORE,InstanceCount=5,InstanceType=m4.large \\\n",
    "    --service-role EMR_DefaultRole \\\n",
    "    --ec2-attributes InstanceProfile=EMR_EC2_DefaultRole,KeyName=<my_key_pair_name> \\\n",
    "    --region \"us-east-1\""
   ],
   "id": "3cb42fea4be513e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "il cluster che è stato utilizzato, è stata scelta questa configuarazione perche con configurazioni poù piccole era quasi impossibile eseguire il Job0, in quanto il cluster impiegavano troppo tempo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Confronto delle Prestazioni**"
   ],
   "id": "864c76d79f2794dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Grafico dei processi](resorc/graf2.png)",
   "id": "cfaf07c43cc49849"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "il Job0 risulta cosi inefficiente rispetto agli altri Jobs, questo è dovuto al fatto che il Job0 esegue un cartesian tra due rdd di grandi dimensioni, questo comporta un grande numero di shuffle e quindi un tempo di esecuzione molto elevato.\n",
    "La sola operazione di cartesian impiega 2,2 h e ha come input 24,9 GB, è il motivo per il quale nelle successive versioni si è cercato di evitare di eseguire questa operazione."
   ],
   "id": "6ad6f47848e5dcb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### grafico delle prestazioni senza Job0",
   "id": "20d23508347a34e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Grafico dei processi](resorc/graf3.png)",
   "id": "56ae58e3869f5831"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "concentrandoci solamente sui Job1, Job2 e Job3 si può notare come la situazione si sia invertita rispetto all'esecuzione locale.",
   "id": "a7015a27703003bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### valutazione Job1\n",
    "\n",
    "nel Job1 risultano esserci stati 5 job ognuno dei quale formato da un solo stage, tranne nel caso della presenza delle operazioni di distinct e aggregateByKey che hanno formato 2 stage, dovuto al fatto che queste operazioni richiedono uno shuffle.\n",
    "\n",
    "Job1 ha uno speed-up di 16,23 rispetto al Job0, questo è dovuto al fatto che si è evitato di eseguire il cartesian tra due rdd di grandi dimensioni, sostituendolo con una broadcast variable.\n",
    "\n",
    "### valutazione Job2\n",
    "\n",
    "nel Job2 risultano esserci stati 2 job, uno per calcolare i vari genere, formato da 2 stage dovuti alla operazione di distinct, e l'altro che contiene tutte le restanti operazioni, formato da 8 stage. Questa differenza rispetto a job1 è dovuta al fatto che non vi è l'utilizzo di broadcast variables.\n",
    "\n",
    "Job2 ha uno speed-up di 20,09 rispetto al Job0, questo è dovuto quasi interamente al fatto che si è evitato di eseguire il cartesian tra due rdd di grandi dimensioni, sostituendolo con una join.\n",
    "\n",
    "Job2 risulta a vere uno speed-up di 1,29 rispetto al Job1, questo è dovuto al fatto che non utilizzando broadcast variables si è riuscito a parallelizzare meglio il calcolo.\n",
    "\n",
    "### valutazione Job3\n",
    "\n",
    "nel job3 risultano esserci stati 3 job, uno per calcolare i vari genere salvati in una broadcast variable, formato da 2 stage dovuti alla operazione di distinct, un altro per valvare in un'altra broadcast variable i le valutazioni dei film e l'altro che contiene tutte le restanti operazioni, formato da 6 stage.\n",
    "\n",
    "Job3 ha uno speed-up di 21,56 rispetto al Job0, dovuto come nei casi precedenti quasi interamente all'aver evitato l'utilizzo della operazione di cartesian.\n",
    "\n",
    "Job3 risulta essere leggermente più efficente del Job2, dovuto all'utilizzo delle broadcast variables dove risultava maggiormente necessario"
   ],
   "id": "2e2cfce516003dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
